
Epoch 1/100, Training Loss: 0.0189, Relative Loss: 0.2276
Epoch 11/100, Training Loss: 0.0031, Relative Loss: 0.0376
Epoch 21/100, Training Loss: 0.0025, Relative Loss: 0.0306
Epoch 31/100, Training Loss: 0.0023, Relative Loss: 0.0274
Epoch 41/100, Training Loss: 0.0021, Relative Loss: 0.0256
Epoch 51/100, Training Loss: 0.0020, Relative Loss: 0.0241
Epoch 61/100, Training Loss: 0.0019, Relative Loss: 0.0233
Epoch 71/100, Training Loss: 0.0019, Relative Loss: 0.0224
Epoch 81/100, Training Loss: 0.0018, Relative Loss: 0.0219
Epoch 91/100, Training Loss: 0.0018, Relative Loss: 0.0217
Test Loss: 0.0016, Relative Loss: 0.0189
Run approximation for model function:
sizes:  torch.Size([99999, 27]) torch.Size([99999, 8]) torch.Size([99999, 27]) torch.Size([99999, 1]) torch.Size([99999, 1])
Epoch 1/100, Training Loss: 0.1825, Relative Loss: 0.4570
Epoch 11/100, Training Loss: 0.0733, Relative Loss: 0.1836
Epoch 21/100, Training Loss: 0.0577, Relative Loss: 0.1445
Epoch 31/100, Training Loss: 0.0440, Relative Loss: 0.1101
Traceback (most recent call last):
  File "/home/baihe_huang/q_func/cs285/scripts/approx.py", line 253, in <module>
    approx.fit_model(model, criterion, optimizer, train_loader, test_loader, total_variance)
  File "/home/baihe_huang/q_func/cs285/scripts/approx.py", line 175, in fit_model
    optimizer.step()
  File "/home/baihe_huang/anaconda3/envs/rl/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/baihe_huang/anaconda3/envs/rl/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/baihe_huang/anaconda3/envs/rl/lib/python3.10/site-packages/torch/optim/adam.py", line 141, in step
    adam(
  File "/home/baihe_huang/anaconda3/envs/rl/lib/python3.10/site-packages/torch/optim/adam.py", line 281, in adam
    func(params,
  File "/home/baihe_huang/anaconda3/envs/rl/lib/python3.10/site-packages/torch/optim/adam.py", line 345, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt